## Today I Learned
### 어텐션(Attention) 메커니즘
시퀀스 데이터에서 중요한 부분에 더 많은 가중치를 할당하여 정보를 효율적으로 처리하는 기법.
```
Attention 스코어 계산
Query와 key 간의 유사도를 측정하여 중요도를 계산

\text{score}(Q, K) = Q \cdot K^T
```
>
```
Softmax를 통한 가중치 계산
계산된 Attention 스코어는 Softmax 함수를 통해 확률 분포로 변환되고, 이를 통해 가중치의 합이 1이 되도록 한다.

\alpha_i = \frac{\exp(\text{score}(Q, K_i))}{\sum_{j} \exp(\text{score}(Q, K_j))}
Softmax를 통해 얻어진 가중치를 Value에 곱하여 최종 Attention 출력을 계산

\text{Attention}(Q, K, V) = \sum_{i} \alpha_i V_i
```
### 자연어 처리(NLP) 모델
- 워드 임베딩(Word Embedding) 기법
단어를 고정된 크기의 벡터로 변환. 단어 같의 의미적 유사성을 반영
```
Word2Vec
단어를 벡터로 변환
CBOW(Continuous Bag of Words) : 주변 단어(context)로 중심 단어(target)를 예측
Skip-gram : 중심단어(target)로 주변 단어(context)를 예측
```
```
GloVe(Global Vectors for Word Representation)
단어-단어 공기행렬(word-word co-occurrence matrix)을 사용, 단어 벡터를 학습.
전역적인 통계 정보를 활용하여 단어 간의 의미적 유사성을 반영
```
- 시퀀스 모델링(Sequence Modeling)
순차적인 데이터를 처리하고 예측, 순환 신경망
>
- Transformer
순차적인 데이터를 병렬로 처리. 자연어 처리에 뛰어남
>
- BERT(Bidirectional Encoder Representations from Transformers)
인코더를 기반으로 한 사전 학습된 언어 모델. 양방향으로 문맥을 이해할 수 있어 다양한 자연어 처리 작업에 뛰어남
>

### ResNet(Residual Network)
깊은 신경망을 학습하기 위해 개발.
잔차 학습(Residual Learning)을 도입해 각 층의 출력이 바로 다음 층의 입력으로 전달되지 않고, 이전 층의 입력을 더해줌으로써 기울기 소실 문제를 완화
>
### 이미지 처리 모델
1. ResNet(Residual Network)
매우 깊은 신경망 학습. 잔차 연결
2. VGG
3X3 필터를 사용하여 깊이를 증가시킴
단순하고 규칙적인 구조로 다양한 변형 가능
3. Inception
다양한 크기의 필터를 병렬로 적용
4. YOLO(You Only Look Once)
객체 탐지(Object Detection) 모델로 이미지에서 객체의 위치와 클래스를 동시에 예측
이미지 전체를 한 번에 처리하여 빠르고 정확한 객체 탐지를 수행
5. 이미지 세그멘테이션(Image Segmentation)
이미지의 각 필셀을 클래스 레이블로 분류
`시맨틱 세그멘테이션 : 이미지의 각 픽셀을 클레스 레이블로 분류`
`인스턴스 세그멘테이션 : 같은 클래스 내에서도 개별 객체를 구분`
- FCN(Fully Convolutional Network) : 모든 레이어를 합성곱 레이어로 구성. 픽셀 단위의 예측 수행
- U-Net : U자형 구조. 인코더-디코더 아키텍처를 사용
- Mask R-CNN : 객체 탐지와 인스턴스 세그멘테이션을 동시에 수행
