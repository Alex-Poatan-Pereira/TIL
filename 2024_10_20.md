## Today I Learned
### 어텐션(Attention) 메커니즘
시퀀스 데이터에서 중요한 부분에 더 많은 가중치를 할당하여 정보를 효율적으로 처리하는 기법.
```
Attention 스코어 계산
Query와 key 간의 유사도를 측정하여 중요도를 계산

\text{score}(Q, K) = Q \cdot K^T
```
>
```
Softmax를 통한 가중치 계산
계산된 Attention 스코어는 Softmax 함수를 통해 확률 분포로 변환되고, 이를 통해 가중치의 합이 1이 되도록 한다.

\alpha_i = \frac{\exp(\text{score}(Q, K_i))}{\sum_{j} \exp(\text{score}(Q, K_j))}
Softmax를 통해 얻어진 가중치를 Value에 곱하여 최종 Attention 출력을 계산

\text{Attention}(Q, K, V) = \sum_{i} \alpha_i V_i
```
### 자연어 처리(NLP) 모델
- 워드 임베딩(Word Embedding) 기법
단어를 고정된 크기의 벡터로 변환. 단어 같의 의미적 유사성을 반영
```
Word2Vec
단어를 벡터로 변환
CBOW(Continuous Bag of Words) : 주변 단어(context)로 중심 단어(target)를 예측
Skip-gram : 중심단어(target)로 주변 단어(context)를 예측
```
```
GloVe(Global Vectors for Word Representation)
단어-단어 공기행렬(word-word co-occurrence matrix)을 사용, 단어 벡터를 학습.
전역적인 통계 정보를 활용하여 단어 간의 의미적 유사성을 반영
```
- 시퀀스 모델링(Sequence Modeling)
순차적인 데이터를 처리하고 예측, 순환 신경망
>
- Transformer
순차적인 데이터를 병렬로 처리. 자연어 처리에 뛰어남
>
- BERT(Bidirectional Encoder Representations from Transformers)
인코더를 기반으로 한 사전 학습된 언어 모델. 양방향으로 문맥을 이해할 수 있어 다양한 자연어 처리 작업에 뛰어남
>

### ResNet

### 이미지 처리 모델