## Today I Learned
### Deep Learning
딥러닝은 비선형 추론, 다층 구조의 신경망, 중요한 특징을 자동으로 추출하여 대규모 데이터와 복잡한 문제를 다루는 데 좋은 성능을 발휘한다.
>
#### 퍼셉트론(Perceptron)
인공 신경망의 가장 기본적인 단위
`y = f(\sum_{i=1}^{n} w_i x_i + b)`
>
#### 다층 퍼셉트론(Multi-Layer Perceptron,MLP)
여러 층의 퍼셉트론을 쌓아 올린 신경망 구조
입력층(input layer), 은닉층(hidden layer), 출력층(output layer)
>
#### 활성화 함수
신경망의 각 뉴런에서 입력값을 출력값으로 변환하는 역할.
``ReLU(Rectified Linear Unit) f(x) = \max(0, x)
계산 간단, 기울기 소실 문제를 완화. 하지만 음수 입력에 대해 기울기가 0이 되는 죽은 ReLU 문제가 발생할 수도 있다``
>
``Sigmoid f(x) = \frac{1}{1 + e^{-x}} 
출력 값이 0과 1사이로 제한되어 확률을 표현하기 적합. 기울기 소실문제, 출력값이 0또는 1에 가까워질 때 학습이 느려지는 문제``
>
``Tanh(Hyperbolic Tangent) 
f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
출력값이 -1과 1사이로 제한되어 중심이 0에 가까워짐. 기울기 소실 문제 가능성``
>
#### 손실함수(Loss Function)
모델의 예측 값과 실제 값 사이의 차이를 측정하는 함수
모델의 성능을 평가하고 최적화 알고리즘을 통해 모델을 학습시키는데 사용
>
``MSE(Mean Squared Error) 
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
회귀 문제에 주로 사용. 예측값과 실제값의 차이를 제곱하여 평균을 구함``
>
``Cross-Entropy \text{Cross-Entropy} = -\sum_{i=1}^{n} y_i \log(\hat{y}_i)
분류 문제에 주로 사용. 예측확률과 실제 클래스 간의 차이를 측정``
>
#### 최적화 알고리즘(Optimization Algorithm)
손실 함수를 최소화하기 위해 모델의 가중치를 조절하는 방법(손실 함수의 기울기를 계산하고 이를 바탕으로 가중치를 업데이트)
>
``SGD(Stochastic Gradient Descent)
전체 데이터셋 대신 무작위로 선택된 일부 데이터(미니배치)를 사용하여 기울기를 계산하고 가중치를 업데이트.
계산이 빠르고 큰 데이터셋에서도 효율적. 최적점에 도달하기까지 진동이 발생할 수도 있다``
>
``Adam(Adaptive Moment Estimation)
모멘텀과 RMSProp을 결합한 알고리즘.
빠른 수렴 속도와 안정적인 학습. 하이퍼파라미터 설정이 복잡할 수 있다.``
>
#### 역전파(Backpropagation)
출력에서 입력 방향으로 손실함수의 기울기를 계산하고 이를 바탕으로 가중치를 업데이트



