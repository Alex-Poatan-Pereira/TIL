## Today I Learned
### Muchine Learning 3
**비지도학습**
- 군집화모델
1. k-means clustering
k개의 군집 중심을 랜덤하게 설정하여 각 데이터 포인트를 가장 가까운 군집 중심에 할당한다.
 각 군집의 중심은 해당 군집에 속한 데이터 포인트들의 평균으로 업데이트한다.
 군집 중심이 더 이상 변화하지 않을 때 까지 할당과 업데이트를 반복한다.
 `유클리드 거리(Euclidean Distance)를 사용하여 거리 계산`
 `엘보우 방법`
>
 2. 계층적 군집화(Hierarchical Clustering)
 데이터 포인트를 계층 구조로 그룹화하는 방법.
 `병합 군집화(Agglomerative Clustering)`
 `분할 군집화(Divisive Clustering)`
 >
 3. DBSCAN(Density-Based Clustering of Applications with Noise)
 데이터 밀도가 높은 영역을 군집으로 간주하고, 밀도가 낮은 영역은 노이즈로 처리한다.
 비구형 군집을 탐지할 수 있고, 균집 수를 사전에 지정할 필요가 없다
 `eps : 두 데이터 포인트가 같은 군집에 속하기 위해 가져야 하는 최대거리`
 `min_samples : 한 군집을 형성하기 위해 필요한 최소 데이터 포인트 수`
 >
 4. PCA(Principal Component Analysis, 주성분 분석)
 데이터 분산을 최대한 보존하면서, 데이터의 주요 특징을 추출해 고차원 데이터를 저차원으로 변환하는 차원 축소 기법.
 `공분산 행렬`
 >
 5. t-SNE(t-Distributed Stochastic Neighbor Embedding)
 데이터 포인트 간의 유사성을 보존하면서 고차원 데이터를 저차원으로 변환하는 차원 축소 기법
 >
 6. LDA(Liner Discriminant Analysis, 성형 판별 분석)
 데이터 클래스 간 분산을 최대화하고, 클래스 내 분산을 최소화 하며 차원 축소와 분류를 수행한다
>
**앙상블 학습**
1. 배깅(Bootstrap Aggregating) : 여러 개의 학습 모델을 병렬로 학습시키고, 그 예측 결과를 평균 또는 다수결로 결합
>
2. 부스팅(Boosting) : 여러 개의 약한 학습기를 순차적으로 학습시키고, 그 예측 결과를 결합하여 강한 학습기를 만듦
>
3. 랜덤 포레스트(Random Forest)
배깅 기법을 기반으로 여러 개의 결정 트리를 학습시키고, 그 예측 결과를 결합하여 최종 예측을 수행
>
4. 그래디언트 부스팅 머신(Gradient Boosting Machine, GBM)
이전 모델이 잘못 예측한 데이터 포인트에 가중치를 부여하여, 다음 모델이 이를 더 잘 학습하도록 한다
>
5. XGBoost(eXtreme Gradient Boosting)
여러 개의 결정 트리로 구성되며, 각 결정 트리는 이전 트리의 예측 오류를 보완하는 방식으로 학습.
`병렬 처리, 조기 종료, 정규화`

 