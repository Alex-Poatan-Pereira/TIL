## Today I Learned
### 오토인코더(Autoencoder)
입력 데이터를 압축하고 이를 다시 복원하는 과정을 통해 데이터를 효율적으로 표현하는 비지도 학습 모델
- 인코더(Encoder)
입력 데이터를 저차원(latent space)으로 표현, 중요한 특징을 추출하고 입력 데이터를 압축
- 디코더(Decoder)
인코더에 의해 생성된 저차원 표현을 다시 원래의 고차원 데이터로 복원(최대한 원본과 가깝게)
- 잠재 공간(Latent Space)
인코더에 의해 생성된 저차원 표현 공간
입력 데이터의 중요한 특징만을 포함. 디코더는 이를 이용해 원래 데이터를 복원
`딥 오토인코더(Deep Autoencoder) : 더 깊은 인코더와 디코더 구조. 복잡한 데이터 표현을 학습`
`변분 오토인코더(Variational Autoencoder, VAE) : 확률적 잠재 공간을 사용하여 데이터 분포를 학습`
`희소 오토인코더(Sparse Autoencoder) : 잠재 공간의 표현을 희소하게 유지하여 중요한 특징만을 학습`
`잡음 제거 오토인코더(Denoising Autoencoder) : 입력 데이터에 잡음을 추가하고 이를 제거하는 학습을 통해 데이터 복원 능력을 향상`
>

### 생성형 모델
1. GAN(Generative Adversarial Network)
생성자와 판별자로 구성. 생성자는 가짜 데이터를 생성하고 판별자는 이 데이터가 진짜인지 가짜인지 판별하며 서로 경쟁하여 동시에 학습
- 생성자(Generator)
랜덤 노이즈 벡터를 입력받아 가짜 데이터를 생성
생성된 데이터는 판별자에게 전달되어 진짜 데이터처럼 보이도록 학습
- 판별자(Discriminator)
진짜 데이터와 생성된 가짜 데이터를 입력으로 받아서 이를 구분
진짜 데이터를 1, 가짜 데이터를 0으로 분류하도록 학습
2. VAE(Variational Autoencoder)
인코더는 입력 데이터를 잠재 공간(latent space)으로 매핑하고 디코더는 이 잠재 공간에서 데이터를 다시 원래 공간으로 복원
잠재 공간을 확률 분포로 모델링하여, 새로운 데이터를 생성할 수 있는 능력을 갖추게 된다 
>
### 전이학습(Transfer Learning)
이미 학습된 모델의 지식을 새로운 문제에 적용하는 방법
데이터가 부족한 상황에서 유용. 모델 학습 시간을 단축하고 성능을 향상시킬 수 있다
- 특징 추출기(Feature Extractor) : 사전 학습된 모델의 초기 층을 고정하고, 새로운 데이터에 맞게 마지막 층만 재학습
- 미세조정(Fine-Tuning) : 사전 학습된 모델 전체를 새로운 데이터에 맞게 재학습
>

### 과적합 방지 기법
1. 정규화(Normalization)
입력 데이터의 분포를 일정한 범위로 조정하여, 모델의 학습을 안정화하고 성능을 향상시키는 기법
- 배치 정규화(Batch Normalization) : 각 미니배치의 평균과 분산을 사용하여 정규화. 학습 속도를 높이고 과적합을 방지
- 레이어 정규화(Layer Normalization) : 각 레이어의 뉴런 출력을 정규화
2. 드롭아웃(Dropout)
학습 과정에서 무작위로 뉴런을 비활성화하여 모델의 과적합을 방지. 학습 시에만 적용되며 평가 시에는 모든 뉴런을 활성화
3. 조기 종료(Early Stopping) 기법
검증 데이터의 성능이 더 이상 향상되지 않을 때 학습을 중단하여 과적합을 방지
학습 과정에서 검증 손실이 일정 에포크 동안 감소하지 않으면 학습을 중단
4. 데이터 증강(Data Aumentation) 기법
원본 데이터를 변형하여 새로운 데이터를 생성. 데이터셋을 확장하고 모델의 일반화 성능을 향상시키는 기법
회전, 이동, 크기 조절, 색상 변환 등
>
### 하이퍼파라미터 튜닝
모델 학습 과정에서 사용자가 설정해야 하는 값으로 모델의 성능에 큰 영향을 미친다
- 학습률
모델의 가중치를 업데이트하는 속도를 결정
너무 크면 학습이 불안정해지고, 너무 작으면 학습이 느려짐
- 배치 크기(Batch Size)
한 번의 업데이트에 사용되는 데이터 샘플의 수를 결정
큰 배치는 학습 속도를 높이지만 메모리 사용량이 증가
- 에포크 수(Number of Epochs)
데이터셋을 몇 번 반복하여 학습할지를 결정
너무 적으면 과소적합이 발생, 너무 많으면 과적합이 발생
조기 종료를 사용하여 적절한 에포크 수를 결정할 수 있다
- 모멘텀(Momentum)
이전 기울기를 현재 기울기에 반영하여 학습 속도를 높이고 진동을 줄인다
- 가중치 초기화(Weight Initialization)
Xavier, He 초기화 등
- Grid Search
하이퍼파라미터의 모든 조합을 시도하여 최적의 값을 찾는다
계산 비용이 많이 들지만 모든 조합을 탐색
- Bayesian Optimization
이전 평가 결과를 바탕으로 다음 평가할 하이퍼파라미터를 선택
계산 비용이 적고 효율적으로 최적의 값을 찾을 수 있다
>
### 교차검증(Cross-Validation)
모델의 일반화 성능을 평가하기 위해 데이터를 여러번 나누어 학습과 검증을 반복
모델이 과적합되지 않고 새로운 데이터에 대해 잘 일반화되는지 평가하는데 유용
- K-Fold 교차 검증
데이터를 K개의 폴드로 나누고 각 폴드에 대해 학습과 검증을 수행
각 폴드에 대한 검증 결과를 저장하고 최종적으로 평균하여 모델의 성능을 평가
>
### Pytorch
```
#모든 신경망 모델의 기본 클래스
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.layer1 = nn.Linear(10, 20)

    def forward(self, x):
        x = self.layer1(x)
        return x
```
```
#손실함수
loss_fn = nn.CrossEntropyLoss() #분류문제

loss_fn = nn.MSELoss() #회귀문제
```
```
#최적화 알고리즘
optimizer = torch.optim.SGD(model.parameters(), lr=0.01) #확률적 경사 하강법 최적화 알고리즘

optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #Adam 최적화 알고리즘
```
```
#데이터셋 및 데이터로더
#사용자 정의 데이터셋을 만들기 위한 기본 클래스
from torch.utils.data import Dataset

class MyDataset(Dataset):
    def __init__(self, data, targets):
        self.data = data
        self.targets = targets

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.targets[idx]

#미니 배치 학습을 위한 데이터 로더
from torch.utils.data import DataLoader

dataset = MyDataset(data, targets)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
```
```
#데이터 변환
#이미지 데이터 변환을 위한 유틸리티
from torchvision import transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
```
```
#합성곱 신경망(CNN)
#2D 합성곱 레이어
conv_layer = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)
```
```
#순환 신경망(RNN)
#기본 순환 신경망 레이어
rnn_layer = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)

#LSTM 레이어
lstm_layer = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)

#GRU 레이어
gru_layer = nn.GRU(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
```
```
#트랜스포머
#트랜스포머 모델
transformer_model = nn.Transformer(nhead=8, num_encoder_layers=6)

#트랜스포머 인코더 레이어
encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
```
```
#모델 저장
torch.save(model.state_dict(), 'model.pth')

#모델 로드
model.load_state_dict(torch.load('model.pth'))
model.eval()
```
```
#모델을 학습 모드로 설정
model.train()

#모델을 평가 모드로 설정
model.eval()
```


